{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab0e8ae",
   "metadata": {},
   "source": [
    "# MoE based GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182db2a",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import Optional\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from utils import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092e253",
   "metadata": {},
   "source": [
    "## MoE Router Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"Simple router that selects top-k experts per token\"\"\"\n",
    "    def __init__(self, hidden_size: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # Router is a simple linear layer mapping hidden states to expert scores\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        router_logits = self.gate(hidden_states)  # [batch, seq, num_experts]\n",
    "        \n",
    "        # Get top-k experts per token\n",
    "        routing_weights = torch.softmax(router_logits, dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize top-k weights to sum to 1\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return top_k_weights, top_k_indices, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac92fad",
   "metadata": {},
   "source": [
    "## MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer replacing the MLP\"\"\"\n",
    "    def __init__(self, dense_mlp, num_experts: int = 8, top_k: int = 2, drop_ratio: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Hugging Face GPT-2 uses Conv1D, where weight shape is [input, output].\n",
    "        hidden_size = dense_mlp.c_fc.weight.shape[0]\n",
    "        # Create router\n",
    "        self.router = TopKRouter(hidden_size, num_experts, top_k)\n",
    "        \n",
    "        # Create experts by copying the dense MLP weights with optional drop-upcycling\n",
    "        self.experts = nn.ModuleList([\n",
    "            self._copy_mlp_with_drop(dense_mlp, drop_ratio) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def _copy_mlp_with_drop(self, dense_mlp, drop_ratio: float):\n",
    "        \"\"\"\n",
    "        Create a copy of the dense MLP with drop-upcycling.\n",
    "        Re-initializes drop_ratio% of parameters to promote diversity.\n",
    "        \"\"\"\n",
    "        expert = copy.deepcopy(dense_mlp)\n",
    "        \n",
    "        if drop_ratio > 0:\n",
    "            with torch.no_grad():\n",
    "                for name, param in expert.named_parameters():\n",
    "                    # Create a mask for parameters to re-initialize\n",
    "                    mask = torch.rand_like(param) < drop_ratio\n",
    "                    \n",
    "                    # Re-initialize masked parameters with small random values\n",
    "                    if mask.any():\n",
    "                        param.data[mask] = torch.randn_like(param[mask]) * 0.02\n",
    "        \n",
    "        return expert\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        total_tokens = batch_size * seq_len\n",
    "        \n",
    "        # Route tokens\n",
    "        top_k_weights, top_k_indices, router_logits = self.router(hidden_states)\n",
    "        \n",
    "        # Flatten everything\n",
    "        flat_hidden = hidden_states.view(total_tokens, hidden_size)\n",
    "        flat_weights = top_k_weights.view(total_tokens, self.top_k)\n",
    "        flat_indices = top_k_indices.view(total_tokens, self.top_k)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(flat_hidden)\n",
    "        \n",
    "        # Create a dispatch mask: [num_experts, total_tokens * top_k]\n",
    "        # This tells us which expert handles which (token, k) pair\n",
    "        flat_indices_1d = flat_indices.view(-1)  # [total_tokens * top_k]\n",
    "        flat_weights_1d = flat_weights.view(-1)  # [total_tokens * top_k]\n",
    "        \n",
    "        # Token indices repeated for each k\n",
    "        token_indices = torch.arange(total_tokens, device=hidden_states.device)\n",
    "        token_indices = token_indices.unsqueeze(1).expand(-1, self.top_k).reshape(-1)\n",
    "        # token_indices: [total_tokens * top_k]\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find all (token, k) pairs assigned to this expert\n",
    "            expert_mask = (flat_indices_1d == expert_idx)\n",
    "            \n",
    "            if not expert_mask.any():\n",
    "                continue\n",
    "            \n",
    "            # Get unique token indices that route to this expert\n",
    "            expert_token_indices = token_indices[expert_mask]\n",
    "            expert_weights = flat_weights_1d[expert_mask]\n",
    "            \n",
    "            # Get inputs (may have duplicates if top_k > 1 and same token uses expert twice)\n",
    "            expert_input = flat_hidden[expert_token_indices]\n",
    "            \n",
    "            # Run expert\n",
    "            expert_output = self.experts[expert_idx](expert_input)\n",
    "            \n",
    "            # Weight and scatter\n",
    "            weighted_output = expert_output * expert_weights.unsqueeze(-1)\n",
    "            \n",
    "            # Use scatter_add to handle potential duplicates\n",
    "            output.index_add_(0, expert_token_indices, weighted_output)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.view(batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_active_params(model):\n",
    "    \"\"\"Calculate total parameters and active parameters in MoE layers\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate MoE-specific info\n",
    "    moe_info = {\n",
    "        'total_experts': 0,\n",
    "        'active_per_token': 0,\n",
    "        'total_expert_params': 0,\n",
    "        'active_expert_params': 0,\n",
    "    }\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, MoELayer):\n",
    "            moe_info['total_experts'] += module.num_experts\n",
    "            moe_info['active_per_token'] += module.top_k\n",
    "            \n",
    "            # Count expert parameters\n",
    "            expert_params = sum(p.numel() for p in module.experts.parameters())\n",
    "            moe_info['total_expert_params'] += expert_params\n",
    "            moe_info['active_expert_params'] += (expert_params / module.num_experts) * module.top_k\n",
    "    \n",
    "    return total_params, moe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc11953",
   "metadata": {},
   "source": [
    "## Upcycle GPT2 Vanilla Weights to MoE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upcycle_gpt2_to_moe(\n",
    "    model_name: str = 'gpt2',\n",
    "    num_experts: int = 8,\n",
    "    top_k: int = 2,\n",
    "    moe_layers: Optional[list] = None,\n",
    "    drop_ratio: float = 0.0,\n",
    "    match_active_params: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a standard GPT-2 model to MoE architecture\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        num_experts: Number of experts per MoE layer\n",
    "        top_k: Number of experts to activate per token\n",
    "        moe_layers: List of layer indices to convert to MoE (None = all layers)\n",
    "        drop_ratio: Ratio of parameters to re-initialize for drop-upcycling (0.0-1.0)\n",
    "                   0.0 = standard upcycling, 0.1-0.2 recommended for drop-upcycling\n",
    "        match_active_params: If True, automatically adjust num_experts to match\n",
    "                            vanilla model's active parameters (top_k=1, num_experts=1)\n",
    "    \n",
    "    Returns:\n",
    "        Modified GPT2LMHeadModel with MoE layers\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model with LM head\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # If no specific layers specified, convert all layers\n",
    "    if moe_layers is None:\n",
    "        moe_layers = list(range(len(model.transformer.h)))\n",
    "    \n",
    "    # Auto-adjust for fair comparison\n",
    "    if match_active_params:\n",
    "        top_k = 1\n",
    "        print(f\"Fair comparison mode: Setting top_k={top_k} to match vanilla GPT-2 active params\")\n",
    "    \n",
    "    upcycle_type = \"Drop-Upcycling\" if drop_ratio > 0 else \"Standard Upcycling\"\n",
    "    print(f\"Converting layers {moe_layers} to MoE with {num_experts} experts (top-{top_k})\")\n",
    "    print(f\"Using {upcycle_type}\" + (f\" with {drop_ratio*100}% parameter re-initialization\" if drop_ratio > 0 else \"\"))\n",
    "    \n",
    "    # Replace MLPs with MoE layers\n",
    "    for layer_idx in moe_layers:\n",
    "        if layer_idx >= len(model.transformer.h):\n",
    "            print(f\"Warning: Layer {layer_idx} doesn't exist, skipping\")\n",
    "            continue\n",
    "            \n",
    "        original_mlp = model.transformer.h[layer_idx].mlp\n",
    "        \n",
    "        # Replace with MoE layer\n",
    "        model.transformer.h[layer_idx].mlp = MoELayer(\n",
    "            original_mlp,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k,\n",
    "            drop_ratio=drop_ratio\n",
    "        )\n",
    "        \n",
    "        print(f\"Converted layer {layer_idx}\")\n",
    "    \n",
    "    # Print parameter comparison\n",
    "    total_params, moe_info = calculate_active_params(model)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PARAMETER COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original model params:        {original_params:,}\")\n",
    "    print(f\"MoE model total params:       {total_params:,}\")\n",
    "    print(f\"MoE model active params:      {int(original_params + moe_info['active_expert_params']):,}\")\n",
    "    print(f\"\\nPer-layer breakdown:\")\n",
    "    print(f\"  Experts per layer:          {num_experts}\")\n",
    "    print(f\"  Active experts per token:   {top_k}\")\n",
    "    print(f\"  Active ratio:               {top_k}/{num_experts} = {top_k/num_experts:.1%}\")\n",
    "    \n",
    "    if match_active_params:\n",
    "        print(f\"\\nFair comparison mode: Active params ≈ vanilla GPT-2\")\n",
    "    else:\n",
    "        active_ratio = (original_params + moe_info['active_expert_params']) / original_params\n",
    "        print(f\"\\nMoE has {active_ratio:.1f}x active parameters vs vanilla\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89214f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_moe_model(model, save_path):\n",
    "    \"\"\"Save MoE model with custom layers\"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'pytorch_model.bin'))\n",
    "    \n",
    "    # Save config\n",
    "    model.config.save_pretrained(save_path)\n",
    "    \n",
    "    # Save MoE configuration\n",
    "    moe_config = {\n",
    "        'moe_layers': [],\n",
    "        'num_experts': None,\n",
    "        'top_k': None,\n",
    "    }\n",
    "    \n",
    "    for layer_idx, layer in enumerate(model.transformer.h):\n",
    "        if isinstance(layer.mlp, MoELayer):\n",
    "            moe_config['moe_layers'].append(layer_idx)\n",
    "            if moe_config['num_experts'] is None:\n",
    "                moe_config['num_experts'] = layer.mlp.num_experts\n",
    "                moe_config['top_k'] = layer.mlp.top_k\n",
    "    \n",
    "    with open(os.path.join(save_path, 'moe_config.json'), 'w') as f:\n",
    "        json.dump(moe_config, f)\n",
    "    \n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae11447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moe_model(load_path, device='cpu'):\n",
    "    \"\"\"Load MoE model with custom layers\"\"\"\n",
    "    # Load MoE config\n",
    "    with open(os.path.join(load_path, 'moe_config.json'), 'r') as f:\n",
    "        moe_config = json.load(f)\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Convert to MoE architecture\n",
    "    for layer_idx in moe_config['moe_layers']:\n",
    "        original_mlp = base_model.transformer.h[layer_idx].mlp\n",
    "        base_model.transformer.h[layer_idx].mlp = MoELayer(\n",
    "            original_mlp,\n",
    "            num_experts=moe_config['num_experts'],\n",
    "            top_k=moe_config['top_k'],\n",
    "            drop_ratio=0.0\n",
    "        )\n",
    "    \n",
    "    # Load trained weights\n",
    "    state_dict = torch.load(os.path.join(load_path, 'pytorch_model.bin'), map_location=device)\n",
    "    base_model.load_state_dict(state_dict)\n",
    "    \n",
    "    print(f\"Model loaded from {load_path}\")\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_model = upcycle_gpt2_to_moe(\n",
    "        model_name='gpt2',\n",
    "        num_experts=8,\n",
    "        top_k=1,\n",
    "        drop_ratio=0.1,\n",
    "        moe_layers=[1, 3, 5, 7, 9, 11],\n",
    "        match_active_params=True\n",
    "    )\n",
    "\n",
    "save_moe_model(moe_model, './gpt2-moe-upcycled')\n",
    "\n",
    "del moe_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d0c94",
   "metadata": {},
   "source": [
    "## Finetune MoE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_moe_model(\n",
    "    moe_model_path: str = './gpt2-moe-upcycled',\n",
    "    output_dir: str = './gpt2-moe-finetuned',\n",
    "    num_train_steps: int = 5000,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 1e-4,\n",
    "    warmup_steps: int = 200,\n",
    "    gradient_accumulation_steps: int = 8,\n",
    "    save_steps: int = 1000,\n",
    "    max_length: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune the upcycled MoE model\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINE-TUNING MoE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the model using custom loader\n",
    "    print(f\"Loading model from {moe_model_path}...\")\n",
    "    model = load_moe_model(moe_model_path)\n",
    "    \n",
    "    print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(f\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('c4', 'en', split='train', streaming=True)\n",
    "    \n",
    "    # Take limited samples for training\n",
    "    num_samples = num_train_steps * batch_size * gradient_accumulation_steps\n",
    "    dataset_list = []\n",
    "    \n",
    "    print(f\"Collecting {num_samples} samples...\")\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        dataset_list.append(example)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Collected {i + 1}/{num_samples} samples...\")\n",
    "    \n",
    "    dataset = Dataset.from_list(dataset_list)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'], \n",
    "            truncation=True, \n",
    "            max_length=max_length,\n",
    "            padding='max_length'\n",
    "        )\n",
    "    \n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text', 'timestamp', 'url']\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        max_steps=num_train_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        lr_scheduler_type='cosine',\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=3,\n",
    "        logging_steps=100,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total steps: {num_train_steps}\")\n",
    "    print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING FINAL MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    save_moe_model(model, output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = finetune_moe_model(\n",
    "        moe_model_path='./gpt2-moe-upcycled',\n",
    "        output_dir='./gpt2-moe-finetuned',\n",
    "        num_train_steps=3125,\n",
    "        batch_size=4,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=312,\n",
    "    )\n",
    "\n",
    "print(\"Successfully fintetuned model at path ./gpt2-moe-fintuned\")\n",
    "\n",
    "del finetuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a1cf9",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef03a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            params: Iterable[torch.nn.parameter.Parameter],\n",
    "            lr: float = 1e-3,\n",
    "            betas: Tuple[float, float] = (0.9, 0.999),\n",
    "            eps: float = 1e-6,\n",
    "            weight_decay: float = 0.0,\n",
    "            correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                # State should be stored in this dictionary.\n",
    "                state = self.state[p]\n",
    "\n",
    "                # Access hyperparameters from the `group` dictionary.\n",
    "                lr = group[\"lr\"]\n",
    "                eps = group[\"eps\"]\n",
    "                weight_decay = group[\"weight_decay\"]\n",
    "                correct_bias = group[\"correct_bias\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                \"\"\"\n",
    "                TODO-6: Implement the AdamW parameter update for this step.\n",
    "\n",
    "                Implementation hints:\n",
    "                1. Update biased first moment estimate:\n",
    "                    m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "                2. Update biased second raw moment estimate:\n",
    "                    v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
    "                3. Apply bias correction if correct_bias=True:\n",
    "                    m_hat = m_t / (1 - beta1^t)\n",
    "                    v_hat = v_t / (1 - beta2^t)\n",
    "                4. Compute step size:\n",
    "                    step_size = lr (or lr / (1 - beta1^t) if bias correction)\n",
    "                5. Update parameters:\n",
    "                    p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                6. Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
    "                    p = p - lr * weight_decay * p\n",
    "                Reference:\n",
    "                Algorithm 1 in \"Adam: A Method for Stochastic Optimization\"\n",
    "                https://arxiv.org/abs/1412.6980\n",
    "                \"\"\"\n",
    "\n",
    "                m_t = exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                v_t = exp_avg_sq.mul_(beta2).add_(grad.square(), alpha=1 - beta2)\n",
    "\n",
    "                if correct_bias:\n",
    "                    m_hat = m_t.div(1 - beta1**t)\n",
    "                    v_hat = v_t.div(1 - beta2**t)\n",
    "                    step_size = lr\n",
    "                    # FIXME: following the step size in the comments raise assertion error in sanity check\n",
    "                    # step_size = lr / (1 - beta1**t)\n",
    "                else:\n",
    "                    m_hat = exp_avg\n",
    "                    v_hat = exp_avg_sq\n",
    "                    step_size = lr\n",
    "\n",
    "                denom = torch.sqrt(v_hat).add(eps)\n",
    "                update_direction = m_hat.div(denom)\n",
    "                p.data.add_(update_direction, alpha=-step_size)\n",
    "\n",
    "                if weight_decay > 0:\n",
    "                    p.data.add_(p.data, alpha=-lr * weight_decay)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbd1b5",
   "metadata": {},
   "source": [
    "## Load NLI Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    correct = sum(p.lower().strip() == l.lower().strip() for p, l in zip(preds, labels))\n",
    "    return correct / len(labels)\n",
    "\n",
    "def generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_gen_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False  # Greedy decoding\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def evaluate_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader, desc=\"Generating\"):\n",
    "            input_ids = item['input_ids']\n",
    "            gen_text = generate_gpt2(model, tokenizer, input_ids, max_gen_length=max_gen_length, device=device)\n",
    "            pred_label = gen_text.split(\"Label:\")[-1].strip()\n",
    "            all_preds.append(pred_label)\n",
    "            all_labels.extend(item['label_strs'])\n",
    "    acc = compute_accuracy(all_preds, all_labels)\n",
    "    print(f\"Evaluation accuracy: {acc*100:.2f}%\")\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for XNLI (Cross-lingual Natural Language Inference) task.\n",
    "\n",
    "    Supports train, dev, and test splits in a specific language,\n",
    "    tokenizes text inputs for GPT-style models, and optionally subsamples the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        split (str): Dataset split, one of 'train', 'dev', 'test'.\n",
    "        lang (str): Language code (e.g., 'en', 'zh').\n",
    "        tokenizer: A HuggingFace tokenizer to convert text to input IDs.\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "        LABEL2ID (dict): Mapping from textual labels to integer IDs.\n",
    "        ID2LABEL (dict): Reverse mapping from integer IDs to textual labels.\n",
    "        data (pd.DataFrame): The loaded and preprocessed dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        lang=\"en\",\n",
    "        train_path_template=\"XNLI-MT-1.0/multinli/multinli.train.{lang}.tsv\",\n",
    "        test_path=\"XNLI-1.0/xnli.test.tsv\",\n",
    "        dev_path=\"XNLI-1.0/xnli.dev.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.lang = lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split == \"train\":\n",
    "            path = train_path_template.format(lang=lang)\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df.dropna(subset=['premise','hypo','label'])\n",
    "        elif split in [\"dev\", \"test\"]:\n",
    "            path = test_path if split==\"test\" else dev_path\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df[df['language']==lang].copy()\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "\n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.iloc[:n].reset_index(drop=True)\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_xnli_tsv(self, path, split):\n",
    "        \"\"\"\n",
    "        Read an XNLI TSV file and return it as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the TSV file.\n",
    "            split (str): One of \"train\", \"dev\", \"test\" indicating the dataset split.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataset as a DataFrame with appropriate columns.\n",
    "        \"\"\"\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols → {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols → {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single example by index and tokenize it.\n",
    "\n",
    "        For training split:\n",
    "            - Constructs the input as \"Premise: ... Hypothesis: ... Label: ...\"\n",
    "            - Tokenizes the full input.\n",
    "            - Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "\n",
    "        For dev/test split:\n",
    "            - Constructs the input without label as \"Premise: ... Hypothesis: ... Label:\"\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'input_ids', 'attention_mask', 'labels' (train only), 'label_str'\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "        if self.lang == 'zh': # de-tokenize for Chinese\n",
    "            premise = premise.replace(\" \", \"\")\n",
    "            hypo = hypo.replace(\" \", \"\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Collate a batch of examples into padded tensors.\n",
    "\n",
    "        Pads 'input_ids' and 'attention_mask' to the max length in the batch.\n",
    "        Pads 'labels' with -100 if present.\n",
    "        Collects 'label_str' for reference.\n",
    "\n",
    "        Returns:\n",
    "            dict: Padded tensors and label strings for the batch.\n",
    "        \"\"\"\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['input_ids'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['attention_mask'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "\n",
    "        if 'labels' in batch[0]:\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(\n",
    "                [b['labels'] for b in batch],\n",
    "                batch_first=True,\n",
    "                padding_value=-100\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        label_strs = [b['label_str'] for b in batch]\n",
    "\n",
    "        out = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label_strs\": label_strs}\n",
    "        if labels is not None:\n",
    "            out[\"labels\"] = labels\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c531d0",
   "metadata": {},
   "source": [
    "## Load Fresh Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb24722",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "CORRECT_BIAS = True\n",
    "\n",
    "model = load_moe_model('./gpt2-moe-finetuned', device='cuda')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-moe-finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b0340",
   "metadata": {},
   "source": [
    "## English Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUBSET = 1\n",
    "DEV_SUBSET = 1\n",
    "TEST_SUBSET = 1\n",
    "\n",
    "train_dataset = XNLIDataset(\n",
    "    split=\"train\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=TRAIN_SUBSET\n",
    ")\n",
    "\n",
    "dev_dataset = XNLIDataset(\n",
    "    split=\"dev\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=DEV_SUBSET\n",
    ")\n",
    "\n",
    "test_dataset = XNLIDataset(\n",
    "    split=\"test\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=TEST_SUBSET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613914a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=XNLIDataset.collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
    "# Track training progress\n",
    "global_train_losses = []\n",
    "total_train_loss = 0.0\n",
    "total_train_steps = 0\n",
    "print_interval = 10\n",
    "\n",
    "# Track best dev accuracy for model saving\n",
    "# This only works for epoch > 1\n",
    "best_dev_acc = 0.0\n",
    "SAVE_DIR = \"best_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    # Iterate over batches\n",
    "    loop = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)        # [B, seq_len]\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch.get(\"labels\").to(DEVICE)                    # [B, seq_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
    "\n",
    "        \"\"\"\n",
    "        TODO-9: Compute next-token loss from hidden states and update model parameters.\n",
    "\n",
    "        Implementation hints:\n",
    "        1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "        3. Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
    "        4. Backpropagate and update model parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        vocabulary_logits = model.hidden_state_to_token(hidden_states)\n",
    "        shifted_logits = vocabulary_logits[:, :-1, :].contiguous()\n",
    "        shifted_labels = labels[:, 1:].contiguous()\n",
    "        logits_for_loss = shifted_logits.view(-1, VOCAB_SIZE)\n",
    "        labels_for_loss = shifted_labels.view(-1)\n",
    "        loss = criterion(logits_for_loss, labels_for_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_steps += 1\n",
    "        global_train_avg_loss = total_train_loss / total_train_steps\n",
    "        global_train_losses.append(global_train_avg_loss)\n",
    "\n",
    "        loop.set_postfix({'avg_loss': f\"{global_train_avg_loss:.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished | Global Avg Loss: {global_train_avg_loss:.4f}\")\n",
    "\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, dev_loader, max_gen_length=1, device=DEVICE)\n",
    "    save_moe_model(model, \"./gpt2-moe-finetuned-en\")\n",
    "    tokenizer.save_pretrained(\"./gpt2-moe-finetuned-en\")\n",
    "\n",
    "    print(\"Model finetuned on XNLI EN split has been saved to ./gpt2-moe-finetuned-en\")\n",
    "    print(f\"The accuracy of this model is: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
