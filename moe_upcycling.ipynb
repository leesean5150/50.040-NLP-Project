{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab0e8ae",
   "metadata": {},
   "source": [
    "# MoE based GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182db2a",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c36b4673",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from typing import Optional\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092e253",
   "metadata": {},
   "source": [
    "## MoE Router Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"Simple router that selects top-k experts per token\"\"\"\n",
    "    def __init__(self, hidden_size: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # Router is a simple linear layer mapping hidden states to expert scores\n",
    "        self.gate = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        router_logits = self.gate(hidden_states)  # [batch, seq, num_experts]\n",
    "        \n",
    "        # Get top-k experts per token\n",
    "        routing_weights = torch.softmax(router_logits, dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize top-k weights to sum to 1\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return top_k_weights, top_k_indices, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac92fad",
   "metadata": {},
   "source": [
    "## MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer replacing the MLP\"\"\"\n",
    "    def __init__(self, dense_mlp, num_experts: int = 8, top_k: int = 2, drop_ratio: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Hugging Face GPT-2 uses Conv1D, where weight shape is [input, output].\n",
    "        hidden_size = dense_mlp.c_fc.weight.shape[0]\n",
    "        intermediate_size = dense_mlp.c_fc.weight.shape[1]\n",
    "        \n",
    "        # Create router\n",
    "        self.router = TopKRouter(hidden_size, num_experts, top_k)\n",
    "        \n",
    "        # Create experts by copying the dense MLP weights with optional drop-upcycling\n",
    "        self.experts = nn.ModuleList([\n",
    "            self._copy_mlp_with_drop(dense_mlp, drop_ratio) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def _copy_mlp(self, dense_mlp):\n",
    "        \"\"\"Create a copy of the dense MLP for each expert\"\"\"\n",
    "        expert = copy.deepcopy(dense_mlp)\n",
    "        return expert\n",
    "    \n",
    "    def _copy_mlp_with_drop(self, dense_mlp, drop_ratio: float):\n",
    "        \"\"\"\n",
    "        Create a copy of the dense MLP with drop-upcycling.\n",
    "        Re-initializes drop_ratio% of parameters to promote diversity.\n",
    "        \"\"\"\n",
    "        expert = copy.deepcopy(dense_mlp)\n",
    "        \n",
    "        if drop_ratio > 0:\n",
    "            with torch.no_grad():\n",
    "                for name, param in expert.named_parameters():\n",
    "                    # Create a mask for parameters to re-initialize\n",
    "                    mask = torch.rand_like(param) < drop_ratio\n",
    "                    \n",
    "                    # Re-initialize masked parameters with small random values\n",
    "                    if mask.any():\n",
    "                        param.data[mask] = torch.randn_like(param[mask]) * 0.02\n",
    "        \n",
    "        return expert\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        \n",
    "        # Route tokens to experts\n",
    "        top_k_weights, top_k_indices, router_logits = self.router(hidden_states)\n",
    "        \n",
    "        # Flatten batch and sequence dimensions\n",
    "        flat_hidden = hidden_states.view(-1, hidden_size)  # [batch*seq, hidden]\n",
    "        flat_top_k_weights = top_k_weights.view(-1, self.top_k)  # [batch*seq, top_k]\n",
    "        flat_top_k_indices = top_k_indices.view(-1, self.top_k)  # [batch*seq, top_k]\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(flat_hidden)\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find tokens routed to this expert\n",
    "            expert_mask = (flat_top_k_indices == expert_idx).any(dim=-1)\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                # Get tokens for this expert\n",
    "                expert_input = flat_hidden[expert_mask]\n",
    "                expert_output = self.experts[expert_idx](expert_input)\n",
    "                \n",
    "                # Get weights for this expert\n",
    "                # For each token, find which top-k position corresponds to this expert\n",
    "                token_indices = torch.where(expert_mask)[0]\n",
    "                for i, token_idx in enumerate(token_indices):\n",
    "                    # Find where this expert appears in top-k for this token\n",
    "                    expert_positions = (flat_top_k_indices[token_idx] == expert_idx).nonzero(as_tuple=True)[0]\n",
    "                    if len(expert_positions) > 0:\n",
    "                        weight = flat_top_k_weights[token_idx, expert_positions[0]]\n",
    "                        output[token_idx] += weight * expert_output[i]\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        output = output.view(batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_active_params(model):\n",
    "    \"\"\"Calculate total parameters and active parameters in MoE layers\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Calculate MoE-specific info\n",
    "    moe_info = {\n",
    "        'total_experts': 0,\n",
    "        'active_per_token': 0,\n",
    "        'total_expert_params': 0,\n",
    "        'active_expert_params': 0,\n",
    "    }\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, MoELayer):\n",
    "            moe_info['total_experts'] += module.num_experts\n",
    "            moe_info['active_per_token'] += module.top_k\n",
    "            \n",
    "            # Count expert parameters\n",
    "            expert_params = sum(p.numel() for p in module.experts.parameters())\n",
    "            moe_info['total_expert_params'] += expert_params\n",
    "            moe_info['active_expert_params'] += (expert_params / module.num_experts) * module.top_k\n",
    "    \n",
    "    return total_params, moe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc11953",
   "metadata": {},
   "source": [
    "## GPT2 Vanilla to MoE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upcycle_gpt2_to_moe(\n",
    "    model_name: str = 'gpt2',\n",
    "    num_experts: int = 8,\n",
    "    top_k: int = 2,\n",
    "    moe_layers: Optional[list] = None,\n",
    "    drop_ratio: float = 0.0,\n",
    "    match_active_params: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a standard GPT-2 model to MoE architecture\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        num_experts: Number of experts per MoE layer\n",
    "        top_k: Number of experts to activate per token\n",
    "        moe_layers: List of layer indices to convert to MoE (None = all layers)\n",
    "        drop_ratio: Ratio of parameters to re-initialize for drop-upcycling (0.0-1.0)\n",
    "                   0.0 = standard upcycling, 0.1-0.2 recommended for drop-upcycling\n",
    "        match_active_params: If True, automatically adjust num_experts to match\n",
    "                            vanilla model's active parameters (top_k=1, num_experts=1)\n",
    "    \n",
    "    Returns:\n",
    "        Modified model with MoE layers\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model\n",
    "    model = GPT2Model.from_pretrained(model_name)\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # If no specific layers specified, convert all layers\n",
    "    if moe_layers is None:\n",
    "        moe_layers = list(range(len(model.h)))\n",
    "    \n",
    "    # Auto-adjust for fair comparison\n",
    "    if match_active_params:\n",
    "        # For fair comparison: active params = top_k experts active per token\n",
    "        # Original has 1 MLP active per token\n",
    "        # So we want: top_k / num_experts ≈ 1 / 1\n",
    "        # Which means: top_k = 1 (only one expert active at a time)\n",
    "        top_k = 1\n",
    "        print(f\"Fair comparison mode: Setting top_k={top_k} to match vanilla GPT-2 active params\")\n",
    "    \n",
    "    upcycle_type = \"Drop-Upcycling\" if drop_ratio > 0 else \"Standard Upcycling\"\n",
    "    print(f\"Converting layers {moe_layers} to MoE with {num_experts} experts (top-{top_k})\")\n",
    "    print(f\"Using {upcycle_type}\" + (f\" with {drop_ratio*100}% parameter re-initialization\" if drop_ratio > 0 else \"\"))\n",
    "    \n",
    "    # Replace MLPs with MoE layers\n",
    "    for layer_idx in moe_layers:\n",
    "        if layer_idx >= len(model.h):\n",
    "            print(f\"Warning: Layer {layer_idx} doesn't exist, skipping\")\n",
    "            continue\n",
    "            \n",
    "        original_mlp = model.h[layer_idx].mlp\n",
    "        \n",
    "        # Replace with MoE layer\n",
    "        model.h[layer_idx].mlp = MoELayer(\n",
    "            original_mlp,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k,\n",
    "            drop_ratio=drop_ratio\n",
    "        )\n",
    "        \n",
    "        print(f\"Converted layer {layer_idx}\")\n",
    "    \n",
    "    # Print parameter comparison\n",
    "    total_params, moe_info = calculate_active_params(model)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PARAMETER COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original model params:        {original_params:,}\")\n",
    "    print(f\"MoE model total params:       {total_params:,}\")\n",
    "    print(f\"MoE model active params:      {int(original_params + moe_info['active_expert_params']):,}\")\n",
    "    print(f\"\\nPer-layer breakdown:\")\n",
    "    print(f\"  Experts per layer:          {num_experts}\")\n",
    "    print(f\"  Active experts per token:   {top_k}\")\n",
    "    print(f\"  Active ratio:               {top_k}/{num_experts} = {top_k/num_experts:.1%}\")\n",
    "    \n",
    "    if match_active_params:\n",
    "        print(f\"\\nFair comparison mode: Active params ≈ vanilla GPT-2\")\n",
    "    else:\n",
    "        active_ratio = (original_params + moe_info['active_expert_params']) / original_params\n",
    "        print(f\"\\nMoE has {active_ratio:.1f}x active parameters vs vanilla\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51116eca",
   "metadata": {},
   "source": [
    "## Upcycle GPT2 Weights for MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] to MoE with 8 experts (top-2)\n",
      "Converted layer 0\n",
      "Converted layer 1\n",
      "Converted layer 2\n",
      "Converted layer 3\n",
      "Converted layer 4\n",
      "Converted layer 5\n",
      "Converted layer 6\n",
      "Converted layer 7\n",
      "Converted layer 8\n",
      "Converted layer 9\n",
      "Converted layer 10\n",
      "Converted layer 11\n"
     ]
    }
   ],
   "source": [
    "moe_model = upcycle_gpt2_to_moe(\n",
    "        model_name='gpt2',\n",
    "        num_experts=8,\n",
    "        top_k=1,\n",
    "        drop_ratio=0.1,\n",
    "        moe_layers=[1, 3, 5, 7, 9, 11],\n",
    "        match_active_params=True\n",
    "    )\n",
    "\n",
    "moe_model.save_pretrained('./gpt2-moe-upcycled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d0c94",
   "metadata": {},
   "source": [
    "## Initialise Training, Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_moe_model(\n",
    "    moe_model_path: str = './gpt2-moe-upcycled',\n",
    "    output_dir: str = './gpt2-moe-finetuned',\n",
    "    num_train_steps: int = 5000,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 1e-4,\n",
    "    warmup_steps: int = 200,\n",
    "    gradient_accumulation_steps: int = 8,\n",
    "    save_steps: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune the upcycled MoE model\n",
    "    \n",
    "    Args:\n",
    "        moe_model_path: Path to the upcycled MoE model\n",
    "        dataset_name: Dataset to use for training (default: The Pile)\n",
    "        output_dir: Directory to save fine-tuned model\n",
    "        num_train_steps: Total training steps\n",
    "        batch_size: Batch size per device\n",
    "        learning_rate: Learning rate (lower than standard for continued training)\n",
    "        warmup_steps: Number of warmup steps\n",
    "        gradient_accumulation_steps: Gradient accumulation steps\n",
    "        save_steps: Save checkpoint every N steps\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINE-TUNING MoE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the model\n",
    "    print(f\"Loading model from {moe_model_path}...\")\n",
    "    base_model = GPT2Model.from_pretrained(moe_model_path)\n",
    "    \n",
    "    # Create LM head model for training\n",
    "    config = GPT2Config.from_pretrained('gpt2')\n",
    "    lm_model = GPT2LMHeadModel(config)\n",
    "    lm_model.transformer = base_model\n",
    "    \n",
    "    print(f\"✓ Model loaded with {sum(p.numel() for p in lm_model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('c4', 'en', split='train', streaming=True)\n",
    "    dataset = dataset.take(num_train_steps * batch_size * gradient_accumulation_steps)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "    \n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text'] if not isinstance(dataset, type(iter([]))) else []\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        max_steps=num_train_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        lr_scheduler_type='cosine',\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=3,\n",
    "        logging_steps=100,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=lm_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total steps: {num_train_steps}\")\n",
    "    print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING FINAL MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45677e78",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = finetune_moe_model(\n",
    "        moe_model_path='./gpt2-moe-upcycled',\n",
    "        output_dir='./gpt2-moe-finetuned',\n",
    "        num_train_steps=3125,\n",
    "        batch_size=4,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=312,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
